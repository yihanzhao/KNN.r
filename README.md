# KNN.r
KNN linear regression is to select different number of neighbors to construct regression line. With smaller number of neighbors, we will have more accurate prediction of the real model, but higher prediction variance. Larger number of neighbors will present a smoother prediction.
To apply the idea of KNN in regression, what we need to do is to fit “local” linear model with k nearest neighbors for each data points. Here, “local” means that we don’t need to do regression over all dataset, but only need to consider k nearest neighbors around each data points.
With high dimensional and small number data, k-NN is not a good choice. High dimensional data need a great number of parameters to estimate if we use nonparametric method like k-NN; however, we only have limited number of sample sample data, which is far from enough to estimated high dimensional data. Thus, in in this case with small data, we probably need a parametric model with less number number of parameters to predict.
